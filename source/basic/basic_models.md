# Basic models
## Fully connected

## RNN
### Layer
#### LSTM (1997)
Long Short-Term Memory
#### GRU (2014)
Gated recurrent unit  
simplified variant of LSTM
### RNN models
#### seq2seq
#### attention model

## CNN (convolutional neural network)
[CNN](/CNN/index.md)

## Restricted Boltzann Machine, RBM
shallow, 2 layer neural networks. First is visible layer, second is hidden layer with sigmoid.  
probabilistic graphical model
Get embedding from hidden layer  
### Deep Belief Network, DBN (2009)
Stacking unsupervised network (RBMs/autoencoders)
Add classifier  (semi-supervisied, layer-wise pre-training)  
The initial weighting is better-> solve local minimum
application: classification, collaborative filtering, feature learning
(better improve activation function, rarely use now)

## Generative Models
[Generative Models](generative_models.md)

## Siamese network (孿生網路) (1993)
usually used for comparison
![](img/siamese_network.png)
e.g. facenet (with triplet loss, i.e. 3 inputs), DaSIAMPRN
## 3D convolution
[3D Convolutional Neural Networks for Human Action Recognition](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.442.8617&rep=rep1&type=pdf)
change color into frame

## Audio
### Text-toSpeech
### Audio-to-Audio
#### Change voice
VAE encode voice (simply CNN) + person recognition (distinguish person like face) like faceNet
use latent space, take average voice of same people
input: audio in target language and other people voice (could be generated by text-to-speech)  
output: audio in target language and target people voice
usage: translate audio
### Speech Recognition
### Music Generation
### Audio/Music-to-Text