# Attention Mechanism
Attention mechanism m ostly used on RNN for NLP, but SAGAN using self-attention on CNN.
## Types of attention mechanism
### Hard Attention
### Soft Attention
### Self-Attention



## Align and Translate (ICLR 2015)
[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)  
> In this paper, we conjecture that the use of a **fixed-length vector is a bottleneck** in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to **automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word**, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.
## Show, Attend and Tell (ICML 2015)
[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)  
Generate word by word based on image
## Attention is all you need (NIPS 2017)
[Attention is all you need](https://arxiv.org/abs/1706.03762)
## Non-local Neural Networks (CVPR 2018)
[Non-local Neural Networks](https://arxiv.org/pdf/1711.07971.pdf)  
[Caffe code](https://github.com/facebookresearch/video-nonlocal-net)  
spacetime Non-local block
